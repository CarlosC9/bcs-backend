import json
import os

import requests
from billiard.context import Process

# from . import celery_app #uncomment #todo uncomment
from biobarcoding.tasks import celery_app # todo delete
from time import sleep, time

from biobarcoding.common.decorators import celery_wf
# from ..common import check_pid_running #TODO UNCOMMENT
# from ..jobs import JobExecutorAtResourceFactory #TODO Unconmment
from biobarcoding.jobs import JobExecutorAtResourceFactory #todo delete
from biobarcoding.db_models.jobs import Job

"""
Celery tasks CANNOT be debugged in Celery!! (the code is run in a separate process; of course they can be debugged "off-line")
"""

# Send messages
# Refresh queues of jobs (at different computing resources)


@celery_app.task
def add(x, y):
    print(f"NORMAL CELERY TASK: {x}+{y}={x + y}")
    return x + y


@celery_app.task
def periodic_sum(x, y):
    print(f"PERIODIC CELERY TASK: {x}+{y}={x + y}")
    return x + y

# ----------------------------------------------------------------

outfile = "/home/rnebot/Downloads/borrame/log.txt"


def append_text(file: str, s: str):
    with open(file, "a+") as f:
        f.write(f"{s}\n")


# To test:
# - Run "bcs-backend" in the IDE
# - Open three shells in the "bcs-backend" directory:
#   - ./start_local_dev_services.sh
#   - celery -A biobarcoding.tasks.definitions flower [OPTIONAL: to see tasks, http://localhost:5555]
#   - curl -i -XPOST http://localhost:5000/api/jobs/ --data-urlencode "{}" --> (should return immediately)
#     - (to see messages generated by tasks of this workflow) tail -f /home/rnebot/Downloads/borrame/log.txt

wf1 = {
    "prepare": {"": "export"},
    "export": {"": "transfer_data"},
    "transfer_data": {"": "submit"},
    "submit": {"": "wait_until_execution_starts"},
    "wait_until_execution_starts": {
        "": "wait_for_execution_end",
        "error": "error",
        "": "cancel",
        "" :"cleanup"

    },
    "wait_for_execution_end": {
        "": "transfer_data_from",
        "error": "error",
        "": "cancel",
        "" : "cleanup"

    },
    "transfer_data_from": {"": "import"},
    "import": {"": "cleanup"},
    "cleanup": {"": "success"},
    # "success"
    # "error"
    # "cancel"
}


def dummy_func(file, secs):
    print(f"Dummy {file}, {secs}")
    start = time()
    endc = start
    while (endc - start) < secs:
        append_text(file, f"elapsed {endc - start} of {secs}")
        sleep(6)
        endc = time()
    os.exit(0)


# "job_context" for Celery tasks:
#
# "job_id": ...,
# "endpoint_url": ...
# "resource": {
#     "name": "",
#     "jm_type": "",
#     "jm_location": {
#     },
#     "jm_credentials": {
#     }
# }
# "process": {
#     "name": "",
#     "inputs": {
#     }
# }


# job_context = {
# "job_id": "00001",
# "endpoint_url": "http://localhost:8080/"
# "resource": {
#     "name": "beauvoir3",
#     "jm_type": "galaxy",
#     "jm_location": {"url" : "https://beauvoir3.corp.itccanarias.org/"
#     },
#     "jm_credentials": {"api_key" : "af107bf81f146b6746944b9488986822"
#     }
# },
# "process": {
#     "name" : "MSA ClustalW",
#     "inputs":
#     {"parameters":
#          {"ClustaW":
#               {"darna": "PROTEIN"} #param name
#     },
#     "data":
#         {"Input dataset":
#             {
#                 "path": "/home/paula/Documentos/NEXTGENDEM/bcs/bcs-backend/tests/data_test/matK_25taxones_Netgendem_SINalinear.fasta",
#                 "type": "fasta"
#             }
#         }
#      }
# }
# }
#

@celery_app.task(name="prepare")
@celery_wf(celery_app, wf1, "prepare")
def wf1_prepare_workspace(job_context):
    """
    Prepare workspace for execution of a Job
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)

    # Example access RESTful endpoint of "bcs-backend"
    # requests.get(job_context["endpoint_url"]+f"/api/jobs/{job_context['job_id']}")

    # TODO update Job status to "preparing_workspace"
    #  requests.put(job_context["endpoint_url"] + f"/api/jobs/{job_context['job_id']}/status", "preparing_workspace")

    # Get resource manager: ssh, galaxy, other
    job_executor = JobExecutorAtResourceFactory.get(tmp["resource"]["jm_type"], tmp["process"])

    # Launch subprocess if needed
    if "_pid" not in tmp:
        p = Process(target=dummy_func, args=(outfile, 13))
        p.start()
        tmp["_pid"] = p.pid
        append_text(outfile, f"prepare_workspace. PID: {p.pid}")
        job_context = json.dumps(tmp)

    # TODO Task "work"
    append_text(outfile, "prepare_workspace")

    # Wait for subprocess to finish (if needed)
    if check_pid_running(tmp["_pid"]):
        append_text(outfile, "retrying task ...")
        return 3, job_context  # Return a tuple with first element <seconds to wait>, <context> to repeat the task
    else:  # Clear "_pid" if subprocess was launched
        del tmp["_pid"]
        job_context = json.dumps(tmp)
        append_text(outfile, "prepare_workspace FINISHED")
        return job_context  # Return nothing (None) or <context> (if context changed) to move to the default next task


@celery_app.task(name="export")
@celery_wf(celery_app, wf1, "export")
def wf1_export_to_supported_file_formats(job_context: str):
    """
    Prepare input files by exporting data using RESTful services

    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)

    # Example access RESTful endpoint of "bcs-backend"
    # requests.get(job_context["endpoint_url"]+f"/api/jobs/{job_context['job_id']}")

    # TODO update Job status to "preparing_workspace"
    #  requests.put(job_context["endpoint_url"] + f"/api/jobs/{job_context['job_id']}/status", "preparing_workspace")

    # Get resource manager: ssh, galaxy, other
    job_executor = JobExecutorAtResourceFactory.get(tmp["resource"]["jm_type"], tmp["process"])

    append_text(outfile, "export_to_supported_file_formats")
    sleep(2)


@celery_app.task(name="transfer_data")
@celery_wf(celery_app, wf1, "transfer_data")
def wf1_transfer_data_to_resource(job_context: str):
    """
    Transfer data to the compute resource
    :param job_context:
    :return:
    """
    append_text(outfile, "transfer_data_to_resource")
    sleep(2)


@celery_app.task(name="submit")
@celery_wf(celery_app, wf1, "submit")
def wf1_submit(job_context: str):
    """
    Submit job to compute resource
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory()
    job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp['resource'])
    inputs = tmp["process"]
    inv_id = job_executor.submit(tmp['job_id'], inputs)
    tmp['g_id'] = inv_id
    job_context = json.dumps(tmp)
    return job_context


@celery_app.task(name="wait_until_execution_starts")
@celery_wf(celery_app, wf1, "wait_until_execution_starts")
def wf1_wait_until_execution_starts(job_context: str):
    """
    Wait for the job to start executing at the
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory()
    job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp['resource'])
    status = job_executor.job_status(tmp["g_id"])
    if status == 'running':  # pasa siempre or running?
        return job_context
    if status == 'error':
        return 'error', job_context
    else:
        return 3, job_context


@celery_app.task(name="wait_for_execution_end")
@celery_wf(celery_app, wf1, "wait_for_execution_end")
def wf1_wait_for_execution_end(job_context: str):
    """
    Wait for the job to finish execution, knowing it is running
    When finished, it can end successfully or with an error.

    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    job = JobExecutorAtResourceFactory()
    job_executor = job.get(tmp["resource"]["jm_type"], tmp["resource"])
    status = job_executor.job_status(tmp["g_id"])
    if isinstance(status, dict):
        return 'error', job_context
    if status == 'ok':
        return job_context
    else:
        return 3, job_context


@celery_app.task(name="transfer_data_from")
@celery_wf(celery_app, wf1, "transfer_data_from")
def wf1_transfer_data_from_resource(job_context: str):
    """
    Once the computation ends, transfer results from the resource to local
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory()
    job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp["resource"])
    r = job_executor.get_results(tmp["g_id"])
    tmp['results'] = r
    job_context = json.dumps(tmp)
    return job_context


@celery_app.task(name="import")
@celery_wf(celery_app, wf1, "import")
def wf1_import_into_database(job_context: str):
    """
    Import resulting files into the database

    :param job_context:
    :return:
    """
    append_text(outfile, "import_into_database")
    sleep(2)


@celery_app.task(name="cleanup")
@celery_wf(celery_app, wf1, "cleanup")
def wf1_cleanup_workspace(job_context: str):
    """
    Delete workspace at the remote resource

    :param job_context:
    :return:
    """

    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory()
    job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp["resource"])
    job_executor.remove_job_workspace(tmp["job_id"])
    del tmp['g_id']
    job_context = json.dumps(tmp)
    return job_context



@celery_app.task(name="success")
@celery_wf(celery_app, wf1, "success")
def wf1_complete_succesfully(job_context: str):
    """
    Just mark the Job as "completed succesfully"

    :param job_context:
    :return:
    """
    append_text(outfile, "complete_successfully")
    sleep(2)


@celery_app.task(name="error")
@celery_wf(celery_app, wf1, "error")
def wf1_completed_error(job_context: str):
    """
    Mark the Job as "completed with error"

    :param job_context:
    :return:
    """

    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory()
    job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp["resource"])
    status = job_executor.job_status(job_executor["g_id"])
    tmp['error'] = status
    job_context = json.dumps(tmp)
    return  job_context

    # append_text(outfile, "completed_error")
    # sleep(2)


@celery_app.task(name="cancel")
@celery_wf(celery_app, wf1, "cancel")
def wf1_cancelled(job_context: str):
    """
    Mark the Job as "cancelled"

    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory()
    job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp["resource"])
    job_executor.cancel_job(tmp['g_id'])
    return job_context


def main():
    job_context_dict = {
        "job_id": "00003",
        "endpoint_url": "http://localhost:8080/",
        "resource":{"name": "beauvoir3",
                    "jm_type": "galaxy",
                    "jm_location": {"url" : "http://localhost:8080/"},
                    "jm_credentials": {"api_key" : "af107bf81f146b6746944b9488986822"}
                    },
        "process": {"name" : "MSA ClustalW",
                    "inputs":
                        {"parameters":
                             {"ClustalW":{"darna": "PROTEIN"}
                              },
                         "data":{"Input dataset":
                             {"path": "/home/paula/Documentos/NEXTGENDEM/bcs/bcs-backend/tests/data_test/matK_25taxones_Netgendem_SINalinear.fasta",
                              "type": "fasta"
                              }
                                 }
                         }
                    }
    }
    

    def wf1_1(job_context):
        tmp = json.loads(job_context)
        job_executor = JobExecutorAtResourceFactory()
        job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp['resource'])
        # wf1_submit
        inputs = tmp["process"]
        inv_id = job_executor.submit(tmp['job_id'], inputs)
        tmp['g_id'] = inv_id
        job_context = json.dumps(tmp)
        return job_context

    def wf1_2(job_context):
        tmp = json.loads(job_context)
        job = JobExecutorAtResourceFactory()
        job_executor = job.get(tmp["resource"]["jm_type"], tmp["resource"])
        status = job_executor.job_status(tmp["g_id"])
        if status == 'running':  # pasa siempre or running?
            return job_context
        if status == 'error':
            return 'error', job_context
        else:
            return 3, job_context


    def wf1_3(job_context):
        tmp = json.loads(job_context)
        job = JobExecutorAtResourceFactory()
        job_executor = job.get(tmp["resource"]["jm_type"], tmp["resource"])
        status = job_executor.job_status(tmp["g_id"])
        if isinstance(status, dict):
            return 'error', job_context
        if status == 'ok':
            return job_context
        else:
            return 3, job_context

    def wf1_errors(job_context):
        tmp = json.loads(job_context)
        job_executor = JobExecutorAtResourceFactory()
        job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp["resource"])
        status = job_executor.job_status(job_executor["g_id"])
        tmp['error'] = status
        job_context = json.dumps(tmp)
        return job_context


    def wf_results(job_context):
        tmp = json.loads(job_context)
        job_executor = JobExecutorAtResourceFactory()
        job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp["resource"])
        r = job_executor.get_results(tmp["g_id"])
        tmp['results'] = r
        print(tmp['results'])
        job_context = json.dumps(tmp)
        return job_context

    def wf_clean(job_context):
        tmp = json.loads(job_context)
        job = JobExecutorAtResourceFactory()
        job_executor = job.get(tmp["resource"]["jm_type"], tmp["resource"])
        job_executor.remove_job_workspace(tmp["job_id"])
        del tmp['g_id']
        job_context = json.dumps(tmp)
        return job_context
    def wf_cancel(job_context):
        tmp = json.loads(job_context)
        job_executor = JobExecutorAtResourceFactory()
        job_executor = job_executor.get(tmp["resource"]["jm_type"], tmp["resource"])
        job_executor.cancel_job(tmp['g_id'])
        return job_context

    def workflow(dict_input,cancel_job = False):
        job_context = json.dumps(dict_input)
        return_1 = wf1_1(job_context)
        if isinstance(return_1, tuple):
            print('error')
        else:
            return_2 = wf1_2(return_1)
            while isinstance(return_2,tuple):
                return_2 = wf1_2(return_1)
            return_3 = wf1_3(return_2)
            while isinstance(return_3,tuple):
                error, _ = return_3
                if error == 'error':
                    return_3 = wf_cancel(return_2)
                    return_4 = wf_clean(return_3)
                    return return_4
                return_3 = wf1_3(return_2)
            if isinstance(return_3,str):
                return_4 = wf_results(return_3)
                return_5 = wf_clean(return_4)
                return return_5
            else:
                return_6 = wf1_errors(job_context)
                return return_6
            
    print(workflow(job_context_dict,cancel_job = True))






if __name__ == "__main__":
    main()
