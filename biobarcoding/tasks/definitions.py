import json
import os

import requests
from billiard.context import Process

from biobarcoding.rest.file_manager import FilesAPI
from biobarcoding.tasks import celery_app
from time import sleep, time

from biobarcoding.common.decorators import celery_wf
from biobarcoding.common import check_pid_running
from biobarcoding.jobs import JobExecutorAtResourceFactory
from biobarcoding.db_models.jobs import Job
from biobarcoding.common import ROOT
from biobarcoding.rest import bcs_api_base

"""
Celery tasks CANNOT be debugged in Celery!! (the code is run in a separate process; of course they can be debugged "off-line")
"""

MAX_ERRORS = 3
TMP_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)),'tmp')


# Send messages
# Refresh queues of jobs (at different computing resources)

def change_status(tmp, status: str):
    endpoint, job_id = tmp["endpoint"], tmp["job_id"]
    # TODO comprobar existencia de
    url = f"{endpoint}{bcs_api_base}/jobs/{job_id}"
    if tmp["status"] != status:
        cmd = [ "curl","--cookie-jar","bcs-cookies.txt", "-c", "cookies.txt", "-X", "PUT", "-H", "Content-Type: application/json", "-d",
               '{"status":"preparing_workspace"}', url]
        import subprocess
        print(subprocess.run(cmd))
        tmp["status"] = status
        return tmp
    else:
        return tmp


def export(job_context,file) -> object:
    """
    Execute remote client script
    @param url: url of the get
    @param path: local path of the file gotten with curl
    @return: pid: PID of the executed script process
    """
    data = job_context['process']['data']
    tmp_path = os.path.join(TMP_PATH, file) # TODO revisar
    extension = data['type']
    bos_type = data['bo_type']
    selection_dict = {"filter": [{"feature_id": {"op": "in", "unary": data['selection']}}]}
    selection_json = json.dumps(selection_dict)
    endpoint = job_context['endpoint']
    url = f"{endpoint}{bcs_api_base}/bos/{bos_type}.{extension}"
    curl = f"curl --cookie-jar bcs-cookies.txt --cookie bcs-cookies.txt -X GET -d \'{selection_json}\' -H \'Content-Type:application/json\' {url} -o {tmp_path}"
    cmd = f"(nohup bash -c &quot;{curl}&quot; >/tmp/mtest </dev/null 2>/tmp/mtest.err & echo $!; wait $!; echo $?  >> {TMP_PATH}/$!.exit_status)"
    print(cmd)
    popen_pipe = os.popen(cmd)
    pid = popen_pipe.readline().rstrip()
    print(f"PID: {pid}")
    return pid

def export_status(pid):
    exit_status = "none"
    if pid:
        if os.path.isfile(f"{TMP_PATH}/{pid}.exit_status"):
            with open(f"{TMP_PATH}/{pid}.exit_status", "r") as f:
                exit_status = f.readline().strip()
                if exit_status.strip() == "0":
                    exit_status = "ok"
                else:
                    print(f"Error executing get with pid: {pid}. Exit status = {exit_status}")
                    exit_status = ""  # This means error
        else:
            exit_status = "running"

    return exit_status

def is_bos_file(input_file):
    if os.path.exists(input_file): #and check that it is not an error file:
        return True
    else:
        return False


@celery_app.task
def add(x, y):
    print(f"NORMAL CELERY TASK: {x}+{y}={x + y}")
    return x + y


@celery_app.task
def periodic_sum(x, y):
    print(f"PERIODIC CELERY TASK: {x}+{y}={x + y}")
    return x + y


# ----------------------------------------------------------------

def append_text(s: str):
    file = ROOT + "/tests/data_test/celery_log.txt"
    with open(file, "a+") as f:
        f.write(f"{s}\n")


# To test:
# - Run "bcs-backend" in the IDE
# - Open three shells in the "bcs-backend" directory:
#   - ./start_local_dev_services.sh
#   - python3 biobarcoding/rest/main.py
#   - celery -A biobarcoding.tasks.definitions flower [OPTIONAL: to see tasks, http://localhost:5555]
#   - curl -i -XPOST http://localhost:5000/api/jobs/ --data-urlencode "{}" --> (should return immediately)
#     - (to see messages generated by tasks of this workflow) tail -f /home/rnebot/Downloads/borrame/log.txt
#   - curl -i -XPOST http://localhost:5000/api/jobs/ -H "Content-Type: application/json" -d @"/home/daniel/Documentos/GIT/bcs-backend/tests/request_transfer.json"
#   - curl --cookie-jar bcs-cookies.txt --cookie bcs-cookies.txt -XPOST http://localhost:5000/api/jobs/ -H "Content-Type: application/json" -d @"/home/paula/Documentos/NEXTGENDEM/bcs/bcs-backend/tests/galaxy_request.json"
wf1 = {
    "prepare": {"": "export"},
    "export": {"": "transfer_data"},
    "transfer_data": {"": "submit"},
    "submit": {"": "wait_until_execution_starts"},
    "wait_until_execution_starts": {
        "": "wait_for_execution_end",
        "error": "error"

    },
    "wait_for_execution_end": {
        "": "transfer_data_from",
        "error": "error"

    },
    "transfer_data_from": {"": "import"},
    "import": {"": "cleanup"},
    "cleanup": {"": "success"},
    # "success"
    # "error"
    # "cancel"
}


def dummy_func(file, secs):
    print(f"Dummy {file}, {secs}")
    start = time()
    endc = start
    while (endc - start) < secs:
        append_text(f"elapsed {endc - start} of {secs}")
        sleep(6)
        endc = time()
    os.exit(0)


# job_context = {"endpoint_url": "http//:localhost:5000/",
#                "process":
#                    {"inputs":
#                         {"parameters": "...."},
#                          "data": [
#                              {
#                          "name": "fasta.fasta",
#                          "file" : {
#                                 {"bo_type": "collection",
#                                 "ids": ["jkdjdjdjjdjd"],
#                                 "type": "fasta"},
#                                   {
#                                     "bo_type": "path",
#                                       "ids":["...."],
#                                       "type":"fasta"
#                                   },
#                                  {"bo_type": "seq",
#                                   "bo": ["jkdjdjdjjdjd", "jdjdjdjfjdjdj"],
#                                 "type": "fasta"}
#                                 }
#                              }
#                          ]
#                     },
#                     "name": {"........"},
#                "outputs":[{"....."}],
#                "status": "created",
#                "resource": {"......."},
#                "job_id": 60}

@celery_app.task(name="prepare")
@celery_wf(celery_app, wf1, "prepare")
def wf1_prepare_workspace(job_context):
    """
    Prepare workspace for execution of a Job
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    tmp = change_status(tmp, "preparing_workspace")

    # Example access RESTful endpoint of "bcs-backend"
    # requests.get(job_context["endpoint_url"]+f"/api/jobs/{job_context['job_id']}")

    # TODO update Job status to "preparing_workspace"
    #  requests.put(job_context["endpoint_url"] + f"/api/jobs/{job_context['job_id']}/status", "preparing_workspace")

    # Get resource manager: ssh, galaxy, other
    # job_executor = JobExecutorAtResourceFactory()
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    job_executor.create_job_workspace(str(tmp["job_id"]))
    job_context = json.dumps(tmp)
    return job_context


@celery_app.task(name="export")
@celery_wf(celery_app, wf1, "export")
def wf1_export_to_supported_file_formats(job_context: str):
    """
    Prepare input files by exporting data using RESTful services

    :param job_context:
    :return:
    """
    # tmp = json.loads(job_context)
    # tmp = change_status(tmp, "preparing_workspace")

    # Example access RESTful endpoint of "bcs-backend"
    # request files from bos ID: 1,2 and save it in local folder
    # curl --cookie-jar bcs-cookies.txt --cookie bcs-cookies.txt -X GET -d '{"filter": [{"feature_id": {"op": "in",
    # "unary": [1, 2]}}]}' -H "Content-Type:application/json"  http://localhost:5000/api/bos/sequences.fasta -o $TEST_FILES_PATH/test.fasta
    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    transfer_state = tmp.get("transfer_state")
    # TODO acordar el path con Rafa
    base_path_to_tmp = TMP_PATH
    inputs_dir = os.path.join(base_path_to_tmp, str(tmp['job_id']))
    print(f"Transfer state: {transfer_state}")
    if transfer_state:  # ya ha empezado la transferencia
        i = transfer_state["idx"]
        n_errors = transfer_state["n_errors"]
    else:
        i = 0
        n_errors = 0
        # os.mkdir(inputs_dir)
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors, state="download", results_dir=inputs_dir)

    # Number of files to transfer
    n_files = len(tmp['process']['data'])

    if i < n_files and n_errors >= MAX_ERRORS:
        print(f"Transfer error: File {i + 1}")
        job_context = json.dumps(tmp)
        return 3, job_context
    if i == n_files:  # Transfer finished
        print("Transfer finished")
        del tmp["transfer_state"]
        job_context = json.dumps(tmp)
        return job_context
    elif export_status(tmp['pid']) == "running":  # Transfer is being executed
        print("Transfer executing")
        return 1, job_context
    elif export_status(tmp['pid']) == "":
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors + 1, state="download", results_dir=inputs_dir)
        job_context = json.dumps(tmp)
        return None, job_context
    elif is_bos_file(os.path.join(TMP_PATH, job_executor.get_export_path(tmp))):
        print(f"File {i + 1} transferred -> Moving to next")
        tmp['process']['data'][i]['path'] = job_executor.get_export_path(tmp)
        del tmp['process']['data'][i]['selection']
        i += 1
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors, state="download", results_dir=inputs_dir)
        job_context = json.dumps(tmp)
        return None, job_context
    else:  # Transfer file i
        print(f"Begin transfer {i + 1}")
        file = job_executor.get_export_path(tmp)
        data = tmp['process']['data'][i]
        pid = export(data, file)
        tmp["pid"] = pid
        tmp["transfer_state"] = dict(idx=i, n_errors=0, state="download", results_dir=inputs_dir)
        job_context = json.dumps(tmp)
        return None, job_context

# job_context = {"endpoint_url": "http//:localhost:5000/",
#                "process":
#                    {"inputs":
#                         {"parameters": {"ClustalW": {"darna": "PROTEIN"}},
#                          "data": [
#                              {"path": "...."},
#                              { "path": "path"},
#                              {"type": "fasta"}
#                          ]},
#                     "name": "MSA ClustalW"},
#                "status": "created",
#                "resource": {".........."},
#                "job_id": 60}


@celery_app.task(name="transfer_data")
@celery_wf(celery_app, wf1, "transfer_data")
def wf1_transfer_data_to_resource(job_context: object) -> object:
    """
    Transfer data to the compute resource
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    tmp = change_status(tmp, "transfer_data_to_resource")
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    print(tmp)
    transfer_state = tmp.get("transfer_state")
    print(f"Transfer state: {transfer_state}")
    if transfer_state:  # ya ha empezado la transferencia
        i = transfer_state["idx"]
        n_errors = transfer_state["n_errors"]
    else:
        i = 0
        n_errors = 0
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors, state="upload")

    # Number of files to transfer
    n_files = len(job_executor.get_upload_files_list(tmp))

    if i < n_files and n_errors >= MAX_ERRORS:
        print(f"File {i + 1} doesn't exist.")
        tmp["transfer_state"] = f"File {i + 1} doesn't exist"
        job_context = json.dumps(tmp)
        return 3, job_context
    elif i == n_files:  # Transfer finished
        print("Transfer finished")
        del tmp["transfer_state"]
        job_context = json.dumps(tmp)
        return job_context
    elif job_executor.job_status(tmp) == "running":  # Transfer is being executed
        print("Transfer executing")
        sleep(1)
        return None, job_context
    elif job_executor.job_status(tmp) == "":  # Transfer error
        print(f"Transfer error: File {i + 1}")
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors + 1, state="upload")
        job_context = json.dumps(tmp)
        return None, job_context
    elif job_executor.exists(tmp):  # File i has been transferred successfully
        print(f"File {i + 1} transferred -> Moving to next")
        i += 1
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors, state="upload")
        job_context = json.dumps(tmp)
        return None, job_context
    else:  # Transfer file i
        print(f"Begin transfer {i + 1}.")
        pid = job_executor.upload_file(tmp)
        tmp["pid"] = pid
        tmp["transfer_state"] = dict(idx=i, n_errors=0, state="upload")
        print(tmp['transfer_state'])
        job_context = json.dumps(tmp)
        return None, job_context


@celery_app.task(name="submit")
@celery_wf(celery_app, wf1, "submit")
def wf1_submit(job_context: str):
    """
    Submit job to compute resource
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    tmp = change_status(tmp, "submit")
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    pid = job_executor.submit(tmp["process"])
    tmp['pid'] = pid
    job_context = json.dumps(tmp)
    append_text(f"submit. workspace: {tmp['pid']}")
    return job_context


@celery_app.task(name="wait_until_execution_starts")
@celery_wf(celery_app, wf1, "wait_until_execution_starts")
def wf1_wait_until_execution_starts(job_context: str):
    """
    Wait for the job to start executing at the
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    status = job_executor.job_status(tmp)
    append_text(f"wait_until_execution_starts: status: {status}")
    if isinstance(status, dict):
        tmp['process']['error'] = status
        job_context = json.dumps(tmp)
        return 'error', job_context
    elif status == 'running' or status == 'ok':
        return job_context
    else:
        return 3, job_context


@celery_app.task(name="wait_for_execution_end")
@celery_wf(celery_app, wf1, "wait_for_execution_end")
def wf1_wait_for_execution_end(job_context: str):
    """
    Wait for the job to finish execution, knowing it is running
    When finished, it can end successfully or with an error.

    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    status = job_executor.job_status(tmp)
    append_text(f"wait_for_execution_end: status: {status}")
    if isinstance(status, dict):
        tmp['process']['error'] = status
        job_context = json.dumps(tmp)
        return 'error', job_context
    elif status == 'ok':
        return job_context
    else:
        return 3, job_context


@celery_app.task(name="transfer_data_from")
@celery_wf(celery_app, wf1, "transfer_data_from")
def wf1_transfer_data_from_resource(job_context: str):
    """
    Once the computation ends, transfer results from the resource to local
    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    transfer_state = tmp.get("transfer_state")
    # TODO acordar el path con Rafa
    base_path_to_results = '/tmp/'
    results_dir = os.path.join(base_path_to_results, str(tmp['job_id']))
    print(f"Transfer state: {transfer_state}")
    if transfer_state:  # ya ha empezado la transferencia
        i = transfer_state["idx"]
        n_errors = transfer_state["n_errors"]
    else:
        i = 0
        n_errors = 0
        os.mkdir(results_dir)
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors, state="download", results_dir=results_dir)

    # Number of files to transfer
    n_files = len(job_executor.get_download_files_list(tmp))

    if i < n_files and n_errors >= MAX_ERRORS:
        print(f"Transfer error: File {i + 1}")
        job_context = json.dumps(tmp)
        return 3, job_context
    if i == n_files:  # Transfer finished
        print("Transfer finished")
        del tmp["transfer_state"]
        job_context = json.dumps(tmp)
        return job_context
    elif job_executor.job_status(tmp) == "running":  # Transfer is being executed
        print("Transfer executing")
        return 1, job_context
    elif job_executor.job_status(tmp) == "":
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors + 1, state="download", results_dir=results_dir)
        job_context = json.dumps(tmp)
        return None, job_context
    elif job_executor.exists(tmp):  # File i has been transferred successfully
        print(f"File {i + 1} transferred -> Moving to next")
        # FilesAPI.put(local_path)
        i += 1
        tmp["pid"] = None
        tmp["transfer_state"] = dict(idx=i, n_errors=n_errors, state="download", results_dir=results_dir)
        job_context = json.dumps(tmp)
        return None, job_context
    else:  # Transfer file i
        print(f"Begin transfer {i + 1}")
        pid = job_executor.download_file(tmp)
        tmp["pid"] = pid
        tmp["transfer_state"] = dict(idx=i, n_errors=0, state="download", results_dir=results_dir)
        job_context = json.dumps(tmp)
        return None, job_context


@celery_app.task(name="import")
@celery_wf(celery_app, wf1, "import")
def wf1_import_into_database(job_context: str):
    """
    Import resulting files into the database

    :param job_context:
    :return:
    """
    append_text("import_into_database")
    sleep(2)


@celery_app.task(name="cleanup")
@celery_wf(celery_app, wf1, "cleanup")
def wf1_cleanup_workspace(job_context: str):
    """
    Delete workspace at the remote resource

    :param job_context:
    :return:
    """

    tmp = json.loads(job_context)
    tmp = change_status(tmp, "cleanup")
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    job_executor.remove_job_workspace(str(tmp["job_id"]))
    del tmp['pid']
    job_context = json.dumps(tmp)
    append_text(f"cleanup:")
    return job_context


@celery_app.task(name="success")
@celery_wf(celery_app, wf1, "success")
def wf1_complete_succesfully(job_context: str):
    """
    Just mark the Job as "completed succesfully"

    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    tmp = change_status(tmp, "success")
    append_text("complete_successfully")
    job_context = json.dumps(tmp)
    append_text(f"cleanup:")
    return job_context


@celery_app.task(name="error")
@celery_wf(celery_app, wf1, "error")
def wf1_completed_error(job_context: str):
    """
    Mark the Job as "completed with error"

    :param job_context:
    :return:
    """

    tmp = json.loads(job_context)
    tmp = change_status(tmp, "error")
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    # los errores ya están en tmp o se descargaría en download
    #  para qué usaría la información en status?
    job_executor.remove_job_workspace(str(tmp['job_id']))
    job_context = json.dumps(tmp)
    append_text(f"error: {tmp['error']}")
    return job_context


@celery_app.task(name="cancel")
@celery_wf(celery_app, wf1, "cancel")
def wf1_cancelled(job_context: str):
    """
    Mark the Job as "cancelled"

    :param job_context:
    :return:
    """
    tmp = json.loads(job_context)
    tmp = change_status(tmp, "cancel")
    job_executor = JobExecutorAtResourceFactory().get(tmp)
    job_executor.cancel_job(tmp['pid'])
    job_executor.remove_job_workspace(tmp)
    append_text(f"cancelled")
    return job_context

